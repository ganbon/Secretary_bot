import torch
from transformers import BertForSequenceClassification,BertJapaneseTokenizer
def generate(input,max_length=50):
    emotion=['ğŸ˜¡','ğŸ˜²','ğŸ˜Ÿ','ğŸ˜§','ğŸ˜Š','ğŸ™‚']
    model_path='model_emotion'
    tokenizer=BertJapaneseTokenizer.from_pretrained(model_path)
    train_model=BertForSequenceClassification.from_pretrained(model_path,num_labels=5)
    data=tokenizer(input,max_length=max_length,truncation = True, padding = "longest", return_tensors = "pt")
    output=train_model.forward(
        input_ids=data['input_ids'],
        attention_mask=data['attention_mask'],
        labels=None
    )
    result=output.logits
    score=torch.argmax(result)
    if result[0][score] > 2:
        return emotion[int(score)]
    else:
        return emotion[5]
    
#æ€’ã£ã¦ã‚‹â€¦0,é©šãâ€¦1,å¯‚ã—ã„â€¦ï¼’æ‚²ã—ã„â€¦ï¼“ã€å¬‰ã—ã„â€¦ï¼”
#s=input()
#generate(s)